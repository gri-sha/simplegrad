{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e169edc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p datasets\n",
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -O ../datasets/shakespeare.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec40001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import simplegrad as sg\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00fd7f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "BLOCK_SIZE = 8\n",
    "LEARNING_RATE = 1e-2\n",
    "MAX_ITERS = 3000\n",
    "VAL_INTERVAL = 300\n",
    "VAL_ITERS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a60b53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab. size: 65\n"
     ]
    }
   ],
   "source": [
    "with open(\"../datasets/shakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(\"Vocab. size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a22759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2i = {ch: i for i, ch in enumerate(chars)}\n",
    "i2s = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda string: [s2i[ch] for ch in string]\n",
    "decode = lambda tokens: [i2s[tok] for tok in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ae80a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1003854\n",
      "Val.: 111540\n"
     ]
    }
   ],
   "source": [
    "data = encode(text)\n",
    "split_idx = int(len(data) * 0.9)\n",
    "train_data = data[:split_idx]\n",
    "val_data = data[split_idx:]\n",
    "print(\"Train:\", len(train_data))\n",
    "print(\"Val.:\", len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06962c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramModel(sg.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_table = sg.nn.Embedding(self.vocab_size, self.vocab_size)\n",
    "\n",
    "    def forward(self, context):\n",
    "        return self.embedding_table(context)\n",
    "\n",
    "    def generate(self, context, max_new_tokens=300):\n",
    "        res = [context.values.item()]\n",
    "        current = context\n",
    "        for _ in range(max_new_tokens):\n",
    "            next = sg.Tensor(np.random.choice(range(self.vocab_size), size=1, p=sg.softmax(self.forward(current), dim=-1).values)[0], dtype=\"int8\")\n",
    "            res.append(int(next.values.item()))\n",
    "            current = next\n",
    "        return res\n",
    "\n",
    "\n",
    "model = BigramModel(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72eb1a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_example: (32, 8)\n",
      "y_example: (32, 8, 65)\n"
     ]
    }
   ],
   "source": [
    "def get_batches(split=\"train\"):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    idxs = np.random.randint(low=0, high=len(data) - BLOCK_SIZE, size=(BATCH_SIZE,))\n",
    "    x = sg.Tensor([data[i : i + BLOCK_SIZE] for i in idxs], dtype=\"int8\")\n",
    "    y = [data[i + 1 : i + BLOCK_SIZE + 1] for i in idxs]\n",
    "    y_one_hot = sg.zeros((BATCH_SIZE, BLOCK_SIZE, vocab_size), comp_grad=False)\n",
    "    for b in range(BATCH_SIZE):\n",
    "        for t in range(BLOCK_SIZE):\n",
    "            y_one_hot.values[b, t, y[b][t]] = 1\n",
    "    return x, y_one_hot\n",
    "\n",
    "\n",
    "x_example, y_example = get_batches(\"train\")\n",
    "print(\"x_example:\", x_example.shape)\n",
    "print(\"y_example:\", y_example.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c527cd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_loss():\n",
    "    out = {}\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = np.zeros(VAL_ITERS)\n",
    "        for i in range(VAL_ITERS):\n",
    "            x_batch, y_batch = get_batches(split)\n",
    "            losses[i] = sg.ce_loss(model(x_batch), y_batch).values.item()\n",
    "        out[split] = losses.mean()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb6712ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOtGgKz.uQw:xx'r PRrkBJjQnR$pB-ZbfQpND,Q3 &aOUAl?LaUTO:,K:sdtGSKLB3\n",
      "CnNhT&DwmhRK!rix'YqmTi3JULCTQDTHMJHwaAhRqt\n",
      "w.TfZZ\n",
      ";i zKqxuyWjQgz.rpileyR!-x;Ijta,bMqQ$zkv&hxFYO-F.mosB-tw.eKgzmd\n",
      "VmdPnDANRn&cAcRB, qz.Dkr&'fZDSos!gLk!gsneCHzJIMwygT3Sllj\n",
      "XI&H:,Ka,$PvUte;gPbcx$mFUPH:cRRrUE\n",
      "..qV&M.TEGGOZwlik b'vhKEUTsGVq:cABgx,WlXq:oDAf;&OzJ M'UWj\n",
      "NkSlhQw;IxOhLglr,! hZwlH:?PRUPbvEUFeo3OBhkWj'hcuGO&SuBNtySWjQxWS,QerBRuFNIiQBoLlI\n",
      "PRU\n",
      "D hfS f-lNrZw:gkarS!vIlONkWj;bbsA!NkKWHJc&obYjtQ;giHM'LSesFfSvPH;Lju:DByOXdrYQyXzlbJ\n"
     ]
    }
   ],
   "source": [
    "print(\"\".join(decode(model.generate(context=sg.Tensor(s2i[\"T\"], dtype=\"int8\"), max_new_tokens=500))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad9aeed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': np.float64(4.6429768300056455), 'val': np.float64(4.628729312419892)}\n",
      "{'train': np.float64(2.811148488521576), 'val': np.float64(2.8107107615470888)}\n",
      "{'train': np.float64(2.5411223328113555), 'val': np.float64(2.5552459859848025)}\n",
      "{'train': np.float64(2.5014451587200166), 'val': np.float64(2.5141910099983216)}\n",
      "{'train': np.float64(2.4837530171871185), 'val': np.float64(2.4977975594997406)}\n",
      "{'train': np.float64(2.469354885816574), 'val': np.float64(2.4924345195293425)}\n",
      "{'train': np.float64(2.47144896030426), 'val': np.float64(2.4890839362144472)}\n",
      "{'train': np.float64(2.458518192768097), 'val': np.float64(2.495996721982956)}\n",
      "{'train': np.float64(2.465827556848526), 'val': np.float64(2.4766753911972046)}\n",
      "{'train': np.float64(2.458669012784958), 'val': np.float64(2.493683046102524)}\n"
     ]
    }
   ],
   "source": [
    "optimizer = sg.opt.Adam(model, lr=LEARNING_RATE)\n",
    "\n",
    "for i in range(MAX_ITERS):\n",
    "    if i % VAL_INTERVAL == 0:\n",
    "        eval_loss = estimate_loss()\n",
    "        print(eval_loss)\n",
    "    x_batch, y_batch = get_batches()\n",
    "    loss = sg.ce_loss(model(x_batch), y_batch)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "865f208c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Th nethe casesthonof fowiowo !\n",
      "Wat f wh s w,\n",
      "INy ms illan witor he marricu fsiche ced Ju okisty ofovounongorimy be ther ind he y y, l me uberathitho che.\n",
      "\n",
      "TES:\n",
      "\n",
      "TUSAn, ghy sengurcemu merear lles 'lurpamsiminghe augh withtharcis iler f ce,\n",
      "Towhio be gon:\n",
      "L:\n",
      "Coregrowhll, at h.\n",
      "Torolo ammy jerButhorig; ld'dee IED I font tor the.\n",
      "O,\n",
      "An IICKIESamaro he ce weiswhen jee sho hadoore ghellay nlantapl see th sind m cot, thesu u Loso As tue heghide ianor cousoiunche omoul an\n",
      "\n",
      "Ay; wicow brld tore thes mon si\n"
     ]
    }
   ],
   "source": [
    "print(''.join(decode(model.generate(context=sg.Tensor(s2i[\"T\"], dtype=\"int8\"), max_new_tokens=500))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
